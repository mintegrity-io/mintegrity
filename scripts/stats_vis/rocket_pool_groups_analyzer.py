#!/usr/bin/env python3
"""
Rocket Pool Groups Analyzer
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—ã –∞–¥—Ä–µ—Å–æ–≤, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏—Ö –æ–¥–Ω–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é,
—Å –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∑–∞ 365 –¥–Ω–µ–π —á–µ—Ä–µ–∑ API.

–ó–ê–ü–£–°–ö:
cd mintegrity
python scripts/stats_vis/rocket_pool_groups_analyzer.py

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ Rocket Pool
2. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—ã –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
3. –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥–æ–≥–æ –∞–¥—Ä–µ—Å–∞ –∑–∞ 365 –¥–Ω–µ–π:
   - –û–±—ä–µ–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ USD (—Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–∞–º–∏)
   - Gas fees –∏ —Å—Ä–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã –≥–∞–∑–∞
   - –í–æ–∑—Ä–∞—Å—Ç –∫–æ—à–µ–ª—å–∫–æ–≤ –∏ –¥–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è
   - –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–¥–Ω–∏, –º–µ—Å—è—Ü—ã)
   - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ—à–µ–ª—å–∫–∞–º–∏ –∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏
4. –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø –∏ —Å–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
5. –°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ HTML –æ—Ç—á–µ—Ç—ã
6. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON –∏ CSV —Ñ–æ—Ä–º–∞—Ç–∞—Ö

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
‚Ä¢ ETHERSCAN_API_KEY –≤ .env —Ñ–∞–π–ª–µ
‚Ä¢ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
‚Ä¢ files/rocket_pool_full_graph_90_days.json
"""

import sys
import os
import json
import csv
import pandas as pd
import numpy as np
import time
import requests
from collections import defaultdict, deque
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib –¥–ª—è headless —Å–µ—Ä–≤–µ—Ä–æ–≤
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å backend –±–µ–∑ GUI
import matplotlib.pyplot as plt

from pathlib import Path
from typing import Dict, List, Set, Optional, Any, Tuple
from dataclasses import dataclass, asdict, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import logging

try:
    from tqdm import tqdm
except ImportError:
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è tqdm –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    class tqdm:
        def __init__(self, iterable=None, total=None, desc=None):
            self.iterable = iterable
            self.total = total
            self.n = 0
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def update(self, n=1):
            self.n += n
        def __iter__(self):
            return iter(self.iterable)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
current_file = Path(__file__).resolve()
scripts_dir = current_file.parent.parent  # scripts/
project_root = scripts_dir.parent  # mintegrity/
sys.path.insert(0, str(project_root))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–°–ù–ê–ß–ê–õ–ê!)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
def load_env_file():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env —Ñ–∞–π–ª–∞"""
    env_file = project_root / ".env"
    
    if env_file.exists():
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å python-dotenv –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                from dotenv import load_dotenv
                load_dotenv(env_file)
                return True
            except ImportError:
                # –ï—Å–ª–∏ python-dotenv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤—Ä—É—á–Ω—É—é
                with open(env_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip('"').strip("'")
                            os.environ[key] = value
                return True
        except Exception as e:
            log.warning(f"Failed to load .env file: {e}")
            return False
    
    return False

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
load_env_file()

warnings.filterwarnings('ignore')
plt.style.use('default')

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞
try:
    from scripts.graph.util.transactions_graph_json import load_graph_from_json
    from scripts.graph.analysis.wallet_groups.wallet_grouping import (
        detect_wallet_coordination, 
        identify_wallet_groups
    )
    log.info("Successfully imported project modules")
except ImportError as e:
    log.error(f"Could not import project modules: {e}")
    log.error("Make sure you are running from the mintegrity directory and all modules exist")
    sys.exit(1)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
try:
    from scripts.commons import metadata
    from scripts.commons.tokens_metadata_scraper import fetch_current_token_prices
    from scripts.commons.known_token_list import ETH_TOKENS_WHITELIST
    FULL_ANALYSIS_AVAILABLE = True
    log.info("‚úÖ Full API analysis modules available")
except ImportError as e:
    log.warning(f"‚ö†Ô∏è  Some API analysis modules not available: {e}")
    log.warning("Will use basic graph-based analysis")
    FULL_ANALYSIS_AVAILABLE = False

@dataclass
class WalletStatistics:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–¥—Ä–µ—Å–∞ –∑–∞ 365 –¥–Ω–µ–π"""
    address: str
    address_type: str = None
    total_volume_usd_365d: Optional[float] = None
    total_transactions_365d: Optional[int] = None
    outgoing_transactions_365d: Optional[int] = None
    incoming_transactions_365d: Optional[int] = None
    wallet_age_days: Optional[int] = None
    active_days_365d: Optional[int] = None
    most_active_month_365d: Optional[str] = None
    total_gas_fees_usd_365d: Optional[float] = None
    unique_addresses_interacted_365d: Optional[int] = None
    average_volume_usd_365d: Optional[float] = None
    max_volume_usd_365d: Optional[float] = None
    median_volume_usd_365d: Optional[float] = None
    wallet_interactions_365d: Optional[int] = None
    contract_interactions_365d: Optional[int] = None
    avg_daily_volume_usd_365d: Optional[float] = None
    max_daily_volume_usd_365d: Optional[float] = None
    total_gas_used_365d: Optional[int] = None
    avg_gas_price_gwei_365d: Optional[float] = None
    creation_date: Optional[str] = None
    first_transaction_date: Optional[str] = None
    last_transaction_date: Optional[str] = None
    token_prices_used: Optional[Dict[str, float]] = None
    error: Optional[str] = None

@dataclass
class GroupStatistics:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø—ã –∞–¥—Ä–µ—Å–æ–≤ –∑–∞ 365 –¥–Ω–µ–π"""
    group_id: int
    group_size: int
    addresses: List[str]
    
    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ 365 –¥–Ω–µ–π
    total_volume_usd_365d: float = 0.0
    total_transactions_365d: int = 0
    total_outgoing_transactions_365d: int = 0
    total_incoming_transactions_365d: int = 0
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    avg_volume_per_address_365d: float = 0.0
    avg_transactions_per_address_365d: float = 0.0
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –≥—Ä—É–ø–ø–µ
    max_volume_in_group_365d: float = 0.0
    max_transactions_in_group_365d: int = 0
    
    # –í–æ–∑—Ä–∞—Å—Ç –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    oldest_wallet_age_days: Optional[int] = None
    newest_wallet_age_days: Optional[int] = None
    avg_wallet_age_days: Optional[float] = None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥—Ä—É–ø–ø—ã
    total_active_days_365d: int = 0
    unique_months_active: int = 0
    coordination_score_avg: float = 0.0
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–µ–π –≤ –≥—Ä—É–ø–ø–µ
    layer_wallets_count: int = 0
    storage_wallets_count: int = 0
    regular_wallets_count: int = 0
    contracts_count: int = 0
    
    # Gas –∏ fees
    total_gas_fees_usd_365d: float = 0.0
    avg_gas_fees_per_address_365d: float = 0.0
    
    # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
    internal_transfers_count: int = 0  # –ü–µ—Ä–µ–≤–æ–¥—ã –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
    external_unique_addresses: int = 0  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ –∞–¥—Ä–µ—Å–∞
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    distance_to_root: Optional[int] = None
    error_addresses: List[str] = None

# === –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞–¥—Ä–µ—Å–æ–≤ ===
class BuiltInAddressAnalyzer:
    """–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞–¥—Ä–µ—Å–æ–≤ —Å –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é API"""

    def __init__(self, max_workers: int = 5):
        global FULL_ANALYSIS_AVAILABLE
        self.max_workers = max_workers
        self.price_cache = {}  # –ö–µ—à –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω
        self.current_token_prices = {}
        
        if FULL_ANALYSIS_AVAILABLE:
            try:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º metadata
                metadata.init()
                self.current_token_prices = self._fetch_current_prices()
                log.info(f"Loaded fallback prices for {len(self.current_token_prices)} tokens")
            except Exception as e:
                log.warning(f"Failed to initialize pricing: {e}")
                FULL_ANALYSIS_AVAILABLE = False

    def _fetch_current_prices(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ü–µ–Ω
            token_prices_with_timestamps = fetch_current_token_prices(ETH_TOKENS_WHITELIST)
            
            current_prices = {}
            for token, (timestamp, price) in token_prices_with_timestamps.items():
                current_prices[token] = price
            
            return current_prices
            
        except Exception as e:
            log.warning(f"Failed to fetch current prices via API: {e}")
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—ã –∏–∑ metadata
            fallback_prices = {}
            for token in ETH_TOKENS_WHITELIST:
                try:
                    price = metadata.get_token_price_usd(token, str(int(time.time())))
                    if price > 0:
                        fallback_prices[token] = price
                except:
                    pass
            
            return fallback_prices

    def get_historical_token_price(self, token_symbol: str, timestamp: int) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é —Ü–µ–Ω—É —Ç–æ–∫–µ–Ω–∞"""
        cache_key = f"{token_symbol.upper()}-{timestamp}"
        
        if cache_key in self.price_cache:
            return self.price_cache[cache_key]
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º metadata
        try:
            price = metadata.get_token_price_usd(token_symbol, str(timestamp))
            if price > 0:
                self.price_cache[cache_key] = price
                return price
        except Exception:
            pass
        
        # –í–Ω–µ—à–Ω–∏–π API (Coinbase)
        try:
            token_to_pair = {
                'ETH': 'ETH-USD', 'BTC': 'BTC-USD', 'WETH': 'ETH-USD',
                'USDT': 'USDT-USD', 'USDC': 'USDC-USD', 'DAI': 'DAI-USD',
                'LINK': 'LINK-USD', 'UNI': 'UNI-USD', 'AAVE': 'AAVE-USD'
            }
            
            pair = token_to_pair.get(token_symbol.upper(), 'ETH-USD')
            
            start_time = timestamp - 3600
            end_time = timestamp + 3600
            
            url = f"https://api.exchange.coinbase.com/products/{pair}/candles"
            params = {
                'start': datetime.fromtimestamp(start_time, timezone.utc).isoformat(),
                'end': datetime.fromtimestamp(end_time, timezone.utc).isoformat(),
                'granularity': 3600
            }
            
            import requests
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            candles = response.json()
            if candles:
                closest_candle = min(candles, key=lambda x: abs(x[0] - timestamp))
                price = float(closest_candle[4])  # close price
                self.price_cache[cache_key] = price
                return price
                
        except Exception:
            pass
        
        # Fallback: —Ç–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
        return self.current_token_prices.get(token_symbol.upper(), 
               self.current_token_prices.get('ETH', 2500.0))

    def get_wallet_statistics_etherscan(self, address: str, address_type: str) -> WalletStatistics:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–µ—Ä–µ–∑ Etherscan API"""
        import os
        import requests
        
        etherscan_api_key = os.getenv("ETHERSCAN_API_KEY")
        if not etherscan_api_key:
            return WalletStatistics(
                address=address,
                address_type=address_type,
                error="ETHERSCAN_API_KEY not set"
            )
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞ 365 –¥–Ω–µ–π
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(days=365)
            
            url = "https://api.etherscan.io/api"
            params = {
                "module": "account",
                "action": "txlist",
                "address": address,
                "startblock": 0,
                "endblock": 99999999,
                "page": 1,
                "offset": 10000,
                "sort": "asc",
                "apikey": etherscan_api_key
            }
            
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data["status"] != "1":
                error_msg = data.get('message', 'Unknown error')
                return WalletStatistics(
                    address=address,
                    address_type=address_type,
                    error=f"Etherscan API error: {error_msg}"
                )
            
            transactions = data["result"]
            if not transactions:
                return self._create_empty_stats(address, address_type)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞ 365 –¥–Ω–µ–π
            start_timestamp = int(start_time.timestamp())
            filtered_transactions = [
                tx for tx in transactions 
                if int(tx["timeStamp"]) >= start_timestamp
            ]
            
            if not filtered_transactions:
                return self._create_empty_stats(address, address_type)
            
            return self._analyze_transactions(address, address_type, filtered_transactions, transactions)
            
        except Exception as e:
            return WalletStatistics(
                address=address,
                address_type=address_type,
                error=f"API error: {str(e)}"
            )

    def _analyze_transactions(self, address: str, address_type: str, 
                            transactions_365d: List[Dict], all_transactions: List[Dict]) -> WalletStatistics:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
        
        volumes_usd = []
        outgoing_txs = []
        incoming_txs = []
        daily_volumes = {}
        monthly_volumes = {}
        gas_used_total = 0
        gas_fees_usd_total = 0.0
        unique_addresses = set()
        
        for tx in transactions_365d:
            value_wei = int(tx["value"])
            value_eth = value_wei / 10**18
            timestamp = int(tx["timeStamp"])
            from_addr = tx["from"].lower()
            to_addr = tx["to"].lower()
            
            is_outgoing = from_addr == address.lower()
            is_incoming = to_addr == address.lower()
            
            if is_outgoing:
                outgoing_txs.append(tx)
            if is_incoming:
                incoming_txs.append(tx)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥—è—â–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            if is_outgoing and value_eth > 0:
                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É ETH –Ω–∞ –º–æ–º–µ–Ω—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                eth_price = self.get_historical_token_price('ETH', timestamp)
                value_usd = value_eth * eth_price
                volumes_usd.append(value_usd)
                
                # –î–Ω–µ–≤–Ω–∞—è –∏ –º–µ—Å—è—á–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                tx_date = datetime.fromtimestamp(timestamp, timezone.utc).date()
                month_key = tx_date.strftime('%Y-%m')
                
                daily_volumes[tx_date] = daily_volumes.get(tx_date, 0) + value_usd
                monthly_volumes[month_key] = monthly_volumes.get(month_key, 0) + value_usd
            
            # Gas –∞–Ω–∞–ª–∏–∑
            if is_outgoing:
                gas_used = int(tx.get("gasUsed", 0))
                gas_price = int(tx.get("gasPrice", 0))
                gas_used_total += gas_used
                
                gas_fee_eth = (gas_used * gas_price) / 10**18
                eth_price = self.get_historical_token_price('ETH', timestamp)
                gas_fees_usd_total += gas_fee_eth * eth_price
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∞–¥—Ä–µ—Å–∞
            other_address = to_addr if is_outgoing else from_addr
            unique_addresses.add(other_address)
        
        # –í–æ–∑—Ä–∞—Å—Ç –∫–æ—à–µ–ª—å–∫–∞
        all_timestamps = [int(tx["timeStamp"]) for tx in all_transactions]
        first_timestamp = min(all_timestamps) if all_timestamps else None
        
        first_date = None
        wallet_age_days = None
        if first_timestamp:
            first_date = datetime.fromtimestamp(first_timestamp, timezone.utc)
            wallet_age_days = (datetime.now(timezone.utc) - first_date).days
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_volume = sum(volumes_usd)
        avg_volume = total_volume / len(volumes_usd) if volumes_usd else 0
        max_volume = max(volumes_usd) if volumes_usd else 0
        median_volume = sorted(volumes_usd)[len(volumes_usd)//2] if volumes_usd else 0
        
        active_days = len(daily_volumes)
        avg_daily_volume = sum(daily_volumes.values()) / len(daily_volumes) if daily_volumes else 0
        max_daily_volume = max(daily_volumes.values()) if daily_volumes else 0
        most_active_month = max(monthly_volumes.items(), key=lambda x: x[1])[0] if monthly_volumes else None
        
        # Gas —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        avg_gas_price_gwei = 0.0
        if outgoing_txs:
            total_gas_price_wei = sum(int(tx.get("gasPrice", 0)) for tx in outgoing_txs)
            avg_gas_price_gwei = (total_gas_price_wei / len(outgoing_txs)) / 10**9
        
        return WalletStatistics(
            address=address,
            address_type=address_type,
            creation_date=first_date.isoformat() if first_date else None,
            first_transaction_date=first_date.isoformat() if first_date else None,
            last_transaction_date=datetime.fromtimestamp(max([int(tx["timeStamp"]) for tx in transactions_365d]), timezone.utc).isoformat(),
            wallet_age_days=wallet_age_days,
            total_volume_usd_365d=round(total_volume, 2),
            average_volume_usd_365d=round(avg_volume, 2),
            max_volume_usd_365d=round(max_volume, 2),
            median_volume_usd_365d=round(median_volume, 2),
            total_transactions_365d=len(transactions_365d),
            outgoing_transactions_365d=len(outgoing_txs),
            incoming_transactions_365d=len(incoming_txs),
            unique_addresses_interacted_365d=len(unique_addresses),
            active_days_365d=active_days,
            avg_daily_volume_usd_365d=round(avg_daily_volume, 2),
            max_daily_volume_usd_365d=round(max_daily_volume, 2),
            most_active_month_365d=most_active_month,
            total_gas_used_365d=gas_used_total,
            total_gas_fees_usd_365d=round(gas_fees_usd_total, 2),
            avg_gas_price_gwei_365d=round(avg_gas_price_gwei, 2),
            wallet_interactions_365d=0,  # –£–ø—Ä–æ—â–µ–Ω–æ
            contract_interactions_365d=0,  # –£–ø—Ä–æ—â–µ–Ω–æ
            token_prices_used={'ETH': self.current_token_prices.get('ETH', 0)}
        )

    def _create_empty_stats(self, address: str, address_type: str) -> WalletStatistics:
        """–°–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return WalletStatistics(
            address=address,
            address_type=address_type,
            total_volume_usd_365d=0.0,
            total_transactions_365d=0,
            outgoing_transactions_365d=0,
            incoming_transactions_365d=0
        )

    def analyze_addresses_batch(self, addresses: List[str], graph) -> List[WalletStatistics]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç –∞–¥—Ä–µ—Å–æ–≤"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_address = {}
            
            for address in addresses:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–¥—Ä–µ—Å–∞ –∏–∑ –≥—Ä–∞—Ñ–∞
                address_type = "wallet"
                if address in graph.nodes:
                    node = graph.nodes[address]
                    if hasattr(node, 'type') and str(node.type).upper() == "CONTRACT":
                        address_type = "contract"
                
                future = executor.submit(self.get_wallet_statistics_etherscan, address, address_type)
                future_to_address[future] = address
            
            with tqdm(total=len(addresses), desc="Analyzing addresses") as pbar:
                for future in as_completed(future_to_address):
                    pbar.update(1)
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        address = future_to_address[future]
                        log.warning(f"Failed to analyze {address}: {e}")
                        results.append(WalletStatistics(
                            address=address,
                            address_type="unknown",
                            error=str(e)
                        ))
        
        return results

class RocketPoolGroupsAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥—Ä—É–ø–ø –∞–¥—Ä–µ—Å–æ–≤ Rocket Pool"""
    
    def __init__(self, 
                 graph_file_path: str = "files/rocket_pool_full_graph_90_days.json",
                 addresses_file_path: Optional[str] = None,
                 output_dir: str = "files/rocket_pool_groups_analysis",
                 coordination_threshold: float = 5.0,
                 min_group_size: int = 2,
                 max_workers: int = 5):
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—É—Ç–∏ - –µ—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ, –¥–µ–ª–∞–µ–º –∏—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        if not Path(graph_file_path).is_absolute():
            self.graph_file_path = project_root / graph_file_path
        else:
            self.graph_file_path = Path(graph_file_path)
            
        if addresses_file_path:
            if not Path(addresses_file_path).is_absolute():
                self.addresses_file_path = project_root / addresses_file_path
            else:
                self.addresses_file_path = Path(addresses_file_path)
        else:
            self.addresses_file_path = None
            
        if not Path(output_dir).is_absolute():
            self.output_dir = project_root / output_dir
        else:
            self.output_dir = Path(output_dir)
            
        self.coordination_threshold = coordination_threshold
        self.min_group_size = min_group_size
        self.max_workers = max_workers
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "plots").mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        
        self.graph = None
        self.wallet_groups = []
        self.individual_stats = {}  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
        self.group_stats = []  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø
        
        log.info(f"Initialized Rocket Pool Groups Analyzer")
        log.info(f"Graph file: {self.graph_file_path}")
        log.info(f"Output directory: {self.output_dir}")
        log.info(f"Coordination threshold: {coordination_threshold}")
        log.info(f"Minimum group size: {min_group_size}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞
        if FULL_ANALYSIS_AVAILABLE:
            log.info("üöÄ Full API analysis available (365-day detailed statistics via Etherscan + historical prices)")
        else:
            log.info("üìä Basic analysis available (graph-based statistics only)")
        
        if self.addresses_file_path:
            log.info(f"üìÅ Will use existing addresses file: {self.addresses_file_path}")

    def load_graph(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥—Ä–∞—Ñ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not self.graph_file_path.exists():
            log.error(f"Graph file not found: {self.graph_file_path}")
            log.error("Please ensure the graph file exists or provide correct path with --graph-path")
            sys.exit(1)
            
        log.info(f"Loading graph from {self.graph_file_path}")
        self.graph = load_graph_from_json(str(self.graph_file_path))
        log.info(f"Successfully loaded graph with {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges")

    def detect_wallet_groups(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—ã –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤"""
        log.info("Detecting coordinated wallet groups...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑ wallet_grouping
        coordination_scores = detect_wallet_coordination(self.graph)
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º wallet_metrics –∏–∑ –º–æ–¥—É–ª—è
        from scripts.graph.analysis.wallet_groups.wallet_grouping import wallet_metrics
        
        self.wallet_groups = identify_wallet_groups(
            coordination_scores, 
            wallet_metrics,
            threshold=self.coordination_threshold
        )
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≥—Ä—É–ø–ø—ã –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        self.wallet_groups = [group for group in self.wallet_groups if len(group) >= self.min_group_size]
        
        log.info(f"Found {len(self.wallet_groups)} groups with {self.min_group_size}+ addresses")
        for i, group in enumerate(self.wallet_groups):
            log.info(f"Group {i+1}: {len(group)} addresses")

    def load_or_analyze_individual_addresses(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≥–æ—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        if self.addresses_file_path and self.addresses_file_path.exists():
            log.info(f"Loading existing addresses analysis from {self.addresses_file_path}")
            
            with open(self.addresses_file_path, 'r') as f:
                addresses_data = json.load(f)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            for addr_data in addresses_data:
                if not addr_data.get('error'):
                    self.individual_stats[addr_data['address']] = WalletStatistics(**addr_data)
            
            log.info(f"Loaded statistics for {len(self.individual_stats)} addresses")
            return
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if FULL_ANALYSIS_AVAILABLE:
            log.info("No existing addresses file found. Performing full 365-day analysis via APIs...")
            self._analyze_addresses_with_full_stats()
        else:
            log.warning("Full analysis modules not available. Creating simplified statistics from graph data...")
            self._create_mock_statistics_from_graph()

    def _analyze_addresses_with_full_stats(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–¥—Ä–µ—Å–∞ —Å –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ –≥—Ä—É–ø–ø
        all_group_addresses = set()
        for group in self.wallet_groups:
            all_group_addresses.update(group)
        
        if not all_group_addresses:
            log.warning("No addresses found in groups")
            return
        
        log.info(f"Starting full 365-day analysis for {len(all_group_addresses)} addresses from groups...")
        log.info("This will fetch detailed statistics via Etherscan API with historical prices")
        log.info("This may take several minutes depending on the number of addresses...")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = BuiltInAddressAnalyzer(max_workers=self.max_workers)
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–¥—Ä–µ—Å–∞ –ø–∞–∫–µ—Ç–∞–º–∏
            addresses_list = list(all_group_addresses)
            batch_size = 50
            all_results = []
            
            for i in range(0, len(addresses_list), batch_size):
                batch = addresses_list[i:i + batch_size]
                log.info(f"Processing batch {i//batch_size + 1}/{(len(addresses_list) + batch_size - 1)//batch_size}")
                
                batch_results = analyzer.analyze_addresses_batch(batch, self.graph)
                all_results.extend(batch_results)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
                if i + batch_size < len(addresses_list):
                    time.sleep(1)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in all_results:
                if not result.error:
                    self.individual_stats[result.address] = result
                else:
                    log.warning(f"Failed to analyze {result.address}: {result.error}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            detailed_file = self.output_dir / "data" / f"detailed_addresses_analysis_{timestamp}.json"
            
            with open(detailed_file, 'w', encoding='utf-8') as f:
                json_data = [asdict(result) for result in all_results]
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            
            success_count = len([r for r in all_results if not r.error])
            log.info(f"Completed detailed analysis: {success_count}/{len(all_results)} addresses successfully analyzed")
            log.info(f"Detailed results saved to: {detailed_file}")
            
        except Exception as e:
            log.error(f"Failed to perform detailed analysis: {e}")
            log.warning("Falling back to simplified graph-based statistics...")
            self._create_mock_statistics_from_graph()

    def _create_mock_statistics_from_graph(self):
        """–°–æ–∑–¥–∞–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∞"""
        all_group_addresses = set()
        for group in self.wallet_groups:
            all_group_addresses.update(group)
        
        for address in all_group_addresses:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç –∏–∑ –≥—Ä–∞—Ñ–∞
            total_transactions = 0
            total_volume = 0.0
            
            # –°—á–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥—è—â–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            for (from_addr, to_addr), edge in self.graph.edges.items():
                if from_addr == address:
                    total_transactions += len(edge.transactions)
                    for tx in edge.transactions.values():
                        total_volume += tx.value_usd
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–¥—Ä–µ—Å–∞
            address_type = "wallet"
            if address in self.graph.nodes:
                node = self.graph.nodes[address]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —É–∑–ª–∞
                if hasattr(node, 'type'):
                    if hasattr(node.type, 'name'):
                        address_type = node.type.name.lower()
                    elif str(node.type).upper() == "CONTRACT":
                        address_type = "contract"
            
            self.individual_stats[address] = WalletStatistics(
                address=address,
                address_type=address_type,
                total_volume_usd_365d=total_volume,
                total_transactions_365d=total_transactions,
                outgoing_transactions_365d=total_transactions,
                incoming_transactions_365d=0,
                wallet_age_days=None,
                active_days_365d=None,
                most_active_month_365d=None,
                total_gas_fees_usd_365d=0.0,
                unique_addresses_interacted_365d=None
            )
        
        log.info(f"Created mock statistics for {len(self.individual_stats)} addresses")

    def calculate_group_statistics(self):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã"""
        log.info("Calculating group statistics...")
        
        self.group_stats = []
        
        for group_id, group_addresses in enumerate(self.wallet_groups, 1):
            log.info(f"Processing group {group_id} with {len(group_addresses)} addresses")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∞–¥—Ä–µ—Å–∞, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            valid_addresses = []
            error_addresses = []
            
            for addr in group_addresses:
                if addr in self.individual_stats:
                    valid_addresses.append(addr)
                else:
                    error_addresses.append(addr)
            
            if not valid_addresses:
                log.warning(f"No valid statistics for group {group_id}")
                continue
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            group_stat = self._aggregate_group_statistics(
                group_id, list(group_addresses), valid_addresses, error_addresses
            )
            
            self.group_stats.append(group_stat)
        
        log.info(f"Calculated statistics for {len(self.group_stats)} groups")

    def _aggregate_group_statistics(self, group_id: int, all_addresses: List[str], 
                                   valid_addresses: List[str], error_addresses: List[str]) -> GroupStatistics:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã"""
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
        stats_list = [self.individual_stats[addr] for addr in valid_addresses]
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        group_size = len(all_addresses)
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–º—ã –∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        total_volume = sum(s.total_volume_usd_365d or 0 for s in stats_list)
        total_transactions = sum(s.total_transactions_365d or 0 for s in stats_list)
        total_outgoing = sum(s.outgoing_transactions_365d or 0 for s in stats_list)
        total_incoming = sum(s.incoming_transactions_365d or 0 for s in stats_list)
        
        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        avg_volume_per_address = total_volume / len(valid_addresses) if valid_addresses else 0
        avg_transactions_per_address = total_transactions / len(valid_addresses) if valid_addresses else 0
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        max_volume = max((s.total_volume_usd_365d or 0 for s in stats_list), default=0)
        max_transactions = max((s.total_transactions_365d or 0 for s in stats_list), default=0)
        
        # –í–æ–∑—Ä–∞—Å—Ç –∫–æ—à–µ–ª—å–∫–æ–≤
        ages = [s.wallet_age_days for s in stats_list if s.wallet_age_days]
        oldest_age = max(ages) if ages else None
        newest_age = min(ages) if ages else None
        avg_age = sum(ages) / len(ages) if ages else None
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        total_active_days = sum(s.active_days_365d or 0 for s in stats_list)
        
        # –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        unique_months = set()
        for s in stats_list:
            if s.most_active_month_365d:
                unique_months.add(s.most_active_month_365d)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫–æ—à–µ–ª—å–∫–æ–≤
        contracts_count = sum(1 for s in stats_list if s.address_type == "contract")
        regular_count = len(stats_list) - contracts_count
        
        # Gas fees
        total_gas_fees = sum(s.total_gas_fees_usd_365d or 0 for s in stats_list)
        avg_gas_fees = total_gas_fees / len(valid_addresses) if valid_addresses else 0
        
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã
        internal_transfers = self._count_internal_transfers(valid_addresses)
        
        # –í–Ω–µ—à–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        external_addresses = set()
        for s in stats_list:
            if s.unique_addresses_interacted_365d:
                external_addresses.add(s.address)
        
        return GroupStatistics(
            group_id=group_id,
            group_size=group_size,
            addresses=all_addresses,
            total_volume_usd_365d=total_volume,
            total_transactions_365d=total_transactions,
            total_outgoing_transactions_365d=total_outgoing,
            total_incoming_transactions_365d=total_incoming,
            avg_volume_per_address_365d=avg_volume_per_address,
            avg_transactions_per_address_365d=avg_transactions_per_address,
            max_volume_in_group_365d=max_volume,
            max_transactions_in_group_365d=max_transactions,
            oldest_wallet_age_days=oldest_age,
            newest_wallet_age_days=newest_age,
            avg_wallet_age_days=avg_age,
            total_active_days_365d=total_active_days,
            unique_months_active=len(unique_months),
            regular_wallets_count=regular_count,
            contracts_count=contracts_count,
            total_gas_fees_usd_365d=total_gas_fees,
            avg_gas_fees_per_address_365d=avg_gas_fees,
            internal_transfers_count=internal_transfers,
            external_unique_addresses=len(external_addresses),
            error_addresses=error_addresses
        )

    def _count_internal_transfers(self, addresses: List[str]) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã"""
        internal_count = 0
        address_set = set(addresses)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä—ë–±—Ä–∞ –≥—Ä–∞—Ñ–∞
        for (from_addr, to_addr), edge in self.graph.edges.items():
            if from_addr in address_set and to_addr in address_set:
                internal_count += len(edge.transactions)
        
        return internal_count

    def create_group_volume_distribution(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø–æ –æ–±—ä–µ–º–∞–º"""
        log.info("Creating group volume distribution...")
        
        if not self.group_stats:
            log.warning("No group statistics available")
            return
        
        volumes = [group.total_volume_usd_365d for group in self.group_stats]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∏–Ω—ã –¥–ª—è –≥—Ä—É–ø–ø
        bins = [
            (0, 10_000, "$0-$10K"),
            (10_000, 100_000, "$10K-$100K"),
            (100_000, 1_000_000, "$100K-$1M"),
            (1_000_000, 10_000_000, "$1M-$10M"),
            (10_000_000, float('inf'), "$10M+")
        ]
        
        bin_counts, bin_labels = self._calculate_bins(volumes, bins)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 8))
        colors = ['#3498db', '#2ecc71', '#f1c40f', '#e67e22', '#e74c3c']
        bars = plt.bar(bin_labels, bin_counts, color=colors, alpha=0.8, edgecolor='black')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, count in zip(bars, bin_counts):
            if count > 0:
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(bin_counts) * 0.01,
                        str(count), ha='center', va='bottom', fontweight='bold')
        
        plt.title('Distribution of Groups by Total Volume (365 days)', fontsize=16, fontweight='bold')
        plt.xlabel('Group Volume Range (USD)', fontsize=12)
        plt.ylabel('Number of Groups', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3, axis='y')
        
        total = len(self.group_stats)
        plt.figtext(0.02, 0.98, f'Total Groups: {total}', fontsize=10,
                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "plots" / "group_volume_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_group_size_analysis(self):
        """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –≥—Ä—É–ø–ø"""
        log.info("Creating group size analysis...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Group Size Analysis', fontsize=16, fontweight='bold')
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º –≥—Ä—É–ø–ø
        group_sizes = [group.group_size for group in self.group_stats]
        unique_sizes = sorted(set(group_sizes))
        size_counts = [group_sizes.count(size) for size in unique_sizes]
        
        axes[0, 0].bar(unique_sizes, size_counts, color='#3498db', alpha=0.8, edgecolor='black')
        axes[0, 0].set_title('Distribution by Group Size')
        axes[0, 0].set_xlabel('Group Size (addresses)')
        axes[0, 0].set_ylabel('Number of Groups')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. –û–±—ä–µ–º vs —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
        volumes = [group.total_volume_usd_365d for group in self.group_stats]
        axes[0, 1].scatter(group_sizes, volumes, color='#e74c3c', alpha=0.7, s=50)
        axes[0, 1].set_title('Group Volume vs Size')
        axes[0, 1].set_xlabel('Group Size (addresses)')
        axes[0, 1].set_ylabel('Total Volume (USD)')
        if any(v > 0 for v in volumes):
            axes[0, 1].set_yscale('log')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. –°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –Ω–∞ –∞–¥—Ä–µ—Å –≤ –≥—Ä—É–ø–ø–µ
        avg_volumes = [group.avg_volume_per_address_365d for group in self.group_stats]
        axes[1, 0].scatter(group_sizes, avg_volumes, color='#2ecc71', alpha=0.7, s=50)
        axes[1, 0].set_title('Average Volume per Address vs Group Size')
        axes[1, 0].set_xlabel('Group Size (addresses)')
        axes[1, 0].set_ylabel('Average Volume per Address (USD)')
        if any(v > 0 for v in avg_volumes):
            axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø
        efficiency = np.array(avg_volumes)
        if any(efficiency > 0):
            axes[1, 1].hist(efficiency[efficiency > 0], bins=15, color='#f39c12', alpha=0.7, edgecolor='black')
            axes[1, 1].set_title('Distribution of Group Efficiency')
            axes[1, 1].set_xlabel('Average Volume per Address (USD)')
            axes[1, 1].set_ylabel('Number of Groups')
            axes[1, 1].set_xscale('log')
        else:
            axes[1, 1].text(0.5, 0.5, 'No volume data available', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "plots" / "group_size_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_top_groups_analysis(self):
        """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–æ–ø –≥—Ä—É–ø–ø"""
        log.info("Creating top groups analysis...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Top Groups Analysis', fontsize=16, fontweight='bold')
        
        n_top = min(10, len(self.group_stats))
        
        # 1. –¢–æ–ø –ø–æ –æ–±—ä–µ–º—É
        top_volume = sorted(self.group_stats, key=lambda x: x.total_volume_usd_365d, reverse=True)[:n_top]
        group_ids = [f"Group {g.group_id}\n({g.group_size} addr)" for g in top_volume]
        volumes = [g.total_volume_usd_365d for g in top_volume]
        
        y_pos = np.arange(len(top_volume))
        axes[0, 0].barh(y_pos, volumes, color='#e74c3c', alpha=0.8)
        axes[0, 0].set_yticks(y_pos)
        axes[0, 0].set_yticklabels(group_ids, fontsize=9)
        axes[0, 0].set_xlabel('Total Volume (USD)')
        axes[0, 0].set_title(f'Top {n_top} Groups by Volume')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. –¢–æ–ø –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        top_tx = sorted(self.group_stats, key=lambda x: x.total_transactions_365d, reverse=True)[:n_top]
        group_ids_tx = [f"Group {g.group_id}\n({g.group_size} addr)" for g in top_tx]
        transactions = [g.total_transactions_365d for g in top_tx]
        
        y_pos = np.arange(len(top_tx))
        axes[0, 1].barh(y_pos, transactions, color='#3498db', alpha=0.8)
        axes[0, 1].set_yticks(y_pos)
        axes[0, 1].set_yticklabels(group_ids_tx, fontsize=9)
        axes[0, 1].set_xlabel('Total Transactions')
        axes[0, 1].set_title(f'Top {n_top} Groups by Transactions')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø
        top_efficiency = sorted(self.group_stats, key=lambda x: x.avg_volume_per_address_365d, reverse=True)[:n_top]
        group_ids_eff = [f"Group {g.group_id}\n({g.group_size} addr)" for g in top_efficiency]
        avg_volumes = [g.avg_volume_per_address_365d for g in top_efficiency]
        
        y_pos = np.arange(len(top_efficiency))
        axes[1, 0].barh(y_pos, avg_volumes, color='#2ecc71', alpha=0.8)
        axes[1, 0].set_yticks(y_pos)
        axes[1, 0].set_yticklabels(group_ids_eff, fontsize=9)
        axes[1, 0].set_xlabel('Average Volume per Address (USD)')
        axes[1, 0].set_title(f'Top {n_top} Most Efficient Groups')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –≥—Ä—É–ø–ø
        coordination_data = [(g.group_id, g.group_size, g.internal_transfers_count) for g in self.group_stats]
        coordination_data.sort(key=lambda x: x[2], reverse=True)
        
        if coordination_data:
            top_coord = coordination_data[:n_top]
            group_ids_coord = [f"Group {g[0]}\n({g[1]} addr)" for g in top_coord]
            internal_transfers = [g[2] for g in top_coord]
            
            y_pos = np.arange(len(top_coord))
            axes[1, 1].barh(y_pos, internal_transfers, color='#9b59b6', alpha=0.8)
            axes[1, 1].set_yticks(y_pos)
            axes[1, 1].set_yticklabels(group_ids_coord, fontsize=9)
            axes[1, 1].set_xlabel('Internal Transfers Count')
            axes[1, 1].set_title(f'Top {n_top} Groups by Internal Coordination')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "plots" / "top_groups_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_groups_vs_individuals_comparison(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø vs –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤"""
        log.info("Creating groups vs individuals comparison...")
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–¥—Ä–µ—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –≤—Ö–æ–¥—è—Ç –≤ –≥—Ä—É–ø–ø—ã
        all_group_addresses = set()
        for group in self.wallet_groups:
            all_group_addresses.update(group)
        
        individual_addresses = []
        for addr, stats in self.individual_stats.items():
            if addr not in all_group_addresses:
                individual_addresses.append(stats)
        
        if not individual_addresses:
            log.warning("No individual addresses found for comparison")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Groups vs Individual Addresses Comparison', fontsize=16, fontweight='bold')
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–≤
        group_volumes = [g.total_volume_usd_365d for g in self.group_stats if g.total_volume_usd_365d > 0]
        individual_volumes = [s.total_volume_usd_365d for s in individual_addresses if s.total_volume_usd_365d and s.total_volume_usd_365d > 0]
        
        axes[0, 0].hist([group_volumes, individual_volumes], bins=20, label=['Groups', 'Individuals'], 
                       alpha=0.7, color=['red', 'blue'], edgecolor='black')
        axes[0, 0].set_xlabel('Total Volume (USD)')
        axes[0, 0].set_ylabel('Count')
        axes[0, 0].set_title('Volume Distribution: Groups vs Individuals')
        if group_volumes or individual_volumes:
            axes[0, 0].set_xscale('log')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Box plot —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–≤
        data_to_plot = []
        labels = []
        if group_volumes:
            data_to_plot.append(group_volumes)
            labels.append('Groups')
        if individual_volumes:
            data_to_plot.append(individual_volumes)
            labels.append('Individuals')
        
        if data_to_plot:
            axes[0, 1].boxplot(data_to_plot, labels=labels)
            axes[0, 1].set_ylabel('Total Volume (USD)')
            axes[0, 1].set_title('Volume Distribution Comparison')
            axes[0, 1].set_yscale('log')
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º vs –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º
        group_avg_volumes = [g.avg_volume_per_address_365d for g in self.group_stats if g.avg_volume_per_address_365d > 0]
        
        if group_avg_volumes and individual_volumes:
            axes[1, 0].hist([group_avg_volumes, individual_volumes], bins=15, 
                           label=['Groups (avg per address)', 'Individuals'], 
                           alpha=0.7, color=['orange', 'blue'], edgecolor='black')
            axes[1, 0].set_xlabel('Volume per Address (USD)')
            axes[1, 0].set_ylabel('Count')
            axes[1, 0].set_title('Volume per Address: Groups vs Individuals')
            axes[1, 0].set_xscale('log')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        group_total_volume = sum(group_volumes) if group_volumes else 0
        individual_total_volume = sum(individual_volumes) if individual_volumes else 0
        total_addresses_in_groups = sum(g.group_size for g in self.group_stats)
        
        stats_text = f"""
        Groups Statistics:
        ‚Ä¢ Count: {len(self.group_stats)}
        ‚Ä¢ Total Volume: ${group_total_volume:,.0f}
        ‚Ä¢ Avg Volume per Group: ${group_total_volume/len(self.group_stats) if self.group_stats else 0:,.0f}
        
        Individual Statistics:
        ‚Ä¢ Count: {len(individual_addresses)}
        ‚Ä¢ Total Volume: ${individual_total_volume:,.0f}
        ‚Ä¢ Avg Volume per Individual: ${individual_total_volume/len(individual_addresses) if individual_addresses else 0:,.0f}
        
        Efficiency:
        ‚Ä¢ Groups control {group_total_volume/(group_total_volume+individual_total_volume)*100 if (group_total_volume+individual_total_volume) > 0 else 0:.1f}% of volume
        ‚Ä¢ With {total_addresses_in_groups}/{total_addresses_in_groups+len(individual_addresses)*100 if (total_addresses_in_groups+len(individual_addresses)) > 0 else 0:.1f}% of addresses
        """
        
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='top', 
                        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "plots" / "groups_vs_individuals_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_transaction_volume_bins_for_all_groups(self):
        """–°–æ–∑–¥–∞–µ—Ç bin plot –ø–æ –ø–æ–ª–Ω–æ–º—É –æ–±—ä—ë–º—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –í–°–ï–• –≥—Ä—É–ø–ø"""
        log.info("Creating transaction volume bins for all groups...")
        
        if not self.group_stats:
            log.warning("No group statistics available")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –æ–±—ä—ë–º—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø
        transaction_volumes = [group.total_transactions_365d for group in self.group_stats]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∏–Ω—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        bins = [
            (0, 100, "0-100 tx"),
            (100, 500, "100-500 tx"),
            (500, 1000, "500-1K tx"),
            (1000, 5000, "1K-5K tx"),
            (5000, 10000, "5K-10K tx"),
            (10000, float('inf'), "10K+ tx")
        ]
        
        bin_counts, bin_labels = self._calculate_bins(transaction_volumes, bins)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('Transaction Volume Distribution for All Groups (365 days)', fontsize=16, fontweight='bold')
        
        # 1. Bin chart
        colors = ['#3498db', '#2ecc71', '#f1c40f', '#e67e22', '#e74c3c', '#9b59b6']
        bars = ax1.bar(bin_labels, bin_counts, color=colors, alpha=0.8, edgecolor='black')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, count in zip(bars, bin_counts):
            if count > 0:
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(bin_counts) * 0.01,
                        str(count), ha='center', va='bottom', fontweight='bold')
        
        ax1.set_title('Groups Distribution by Transaction Count')
        ax1.set_xlabel('Transaction Count Range')
        ax1.set_ylabel('Number of Groups')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # 2. Cumulative distribution
        sorted_volumes = sorted(transaction_volumes, reverse=True)
        cumulative_percent = [(i+1)/len(sorted_volumes)*100 for i in range(len(sorted_volumes))]
        
        ax2.plot(sorted_volumes, cumulative_percent, 'o-', color='#e74c3c', linewidth=2, markersize=4)
        ax2.set_title('Cumulative Transaction Distribution')
        ax2.set_xlabel('Total Transactions (365d)')
        ax2.set_ylabel('Cumulative Percentage of Groups (%)')
        ax2.grid(True, alpha=0.3)
        ax2.set_xscale('log')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_groups = len(self.group_stats)
        total_transactions = sum(transaction_volumes)
        avg_transactions = total_transactions / total_groups if total_groups > 0 else 0
        median_transactions = sorted(transaction_volumes)[len(transaction_volumes)//2] if transaction_volumes else 0
        
        stats_text = f"""
        Total Groups: {total_groups}
        Total Transactions: {total_transactions:,}
        Average per Group: {avg_transactions:,.0f}
        Median per Group: {median_transactions:,}
        """
        
        fig.text(0.02, 0.02, stats_text, fontsize=10, 
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "plots" / "transaction_volume_bins_all_groups.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # –ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥
        self._print_bins_stats("Transaction Volume", bin_labels, bin_counts, np.array(transaction_volumes))

    def _print_bins_stats(self, prefix, bin_labels, bin_counts, data):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∏–Ω–∞–º"""
        total = sum(bin_counts)
        log.info(f"üìä {prefix} distribution:")
        for label, count in zip(bin_labels, bin_counts):
            percentage = (count / total * 100) if total > 0 else 0
            log.info(f"  {label}: {count} groups ({percentage:.1f}%)")
        
        log.info(f"üìà {prefix} statistics:")
        log.info(f"  Total: {data.sum():,.0f}")
        log.info(f"  Average: {data.mean():,.0f}")
        log.info(f"  Median: {np.median(data):,.0f}")
        log.info(f"  Max: {data.max():,.0f}")
        log.info(f"  Min: {data.min():,.0f}")

    def generate_groups_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º (–±–µ–∑ HTML –æ—Ç—á–µ—Ç–∞)"""
        log.info("Generating groups statistics summary...")
        
        if not self.group_stats:
            log.warning("No group statistics for report")
            return {}
        
        df_groups = pd.DataFrame([asdict(group) for group in self.group_stats])
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_groups = len(self.group_stats)
        total_addresses_in_groups = df_groups['group_size'].sum()
        total_volume = df_groups['total_volume_usd_365d'].sum()
        total_transactions = df_groups['total_transactions_365d'].sum()
        avg_group_size = df_groups['group_size'].mean()
        avg_volume_per_group = df_groups['total_volume_usd_365d'].mean()
        
        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø
        largest_group = df_groups['group_size'].max()
        most_active_group_id = df_groups.loc[df_groups['total_transactions_365d'].idxmax(), 'group_id']
        highest_volume_group_id = df_groups.loc[df_groups['total_volume_usd_365d'].idxmax(), 'group_id']
        
        stats = {
            'total_groups': total_groups,
            'total_addresses_in_groups': total_addresses_in_groups,
            'avg_group_size': avg_group_size,
            'largest_group_size': largest_group,
            'total_volume': total_volume,
            'avg_volume_per_group': avg_volume_per_group,
            'total_transactions': total_transactions,
            'most_active_group_id': most_active_group_id,
            'highest_volume_group_id': highest_volume_group_id
        }
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥ –≤–º–µ—Å—Ç–æ HTML
        log.info("=" * 50)
        log.info("üìä GROUPS ANALYSIS SUMMARY")
        log.info("=" * 50)
        log.info(f"Total Groups: {stats['total_groups']}")
        log.info(f"Total Addresses in Groups: {stats['total_addresses_in_groups']}")
        log.info(f"Average Group Size: {stats['avg_group_size']:.1f}")
        log.info(f"Largest Group Size: {stats['largest_group_size']}")
        log.info(f"Total Volume: ${stats['total_volume']:,.0f}")
        log.info(f"Average Volume per Group: ${stats['avg_volume_per_group']:,.0f}")
        log.info(f"Total Transactions: {stats['total_transactions']:,.0f}")
        log.info(f"Most Active Group: {stats['most_active_group_id']}")
        log.info(f"Highest Volume Group: {stats['highest_volume_group_id']}")
        log.info("=" * 50)
        
        # –¢–æ–ø 5 –≥—Ä—É–ø–ø –≤ –ª–æ–≥–µ
        log.info("üèÜ TOP 5 GROUPS BY VOLUME:")
        top_5_groups = df_groups.nlargest(5, 'total_volume_usd_365d')
        for i, (_, group) in enumerate(top_5_groups.iterrows(), 1):
            log.info(f"{i}. Group {group['group_id']} - {group['group_size']} addresses")
            log.info(f"   Volume: ${group['total_volume_usd_365d']:,.0f} | Transactions: {group['total_transactions_365d']:,}")
        
        log.info("=" * 50)
        
        return stats

    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø"""
        log.info("=" * 60)
        log.info("ROCKET POOL GROUPS ANALYSIS STARTED")
        log.info("=" * 60)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –∞–Ω–∞–ª–∏–∑–∞
        if FULL_ANALYSIS_AVAILABLE and not (self.addresses_file_path and self.addresses_file_path.exists()):
            log.info("üöÄ FULL ANALYSIS MODE:")
            log.info("   ‚Ä¢ Will fetch 365-day detailed statistics via APIs")
            log.info("   ‚Ä¢ Uses Etherscan API for transaction history") 
            log.info("   ‚Ä¢ Uses Coinbase API for historical token prices")
            log.info("   ‚Ä¢ Includes gas fees, wallet age, activity patterns")
            log.info("")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
            etherscan_api_key = os.getenv("ETHERSCAN_API_KEY")
            if etherscan_api_key:
                masked_key = etherscan_api_key[:8] + "..." + etherscan_api_key[-4:] if len(etherscan_api_key) > 12 else "***"
                log.info(f"‚úÖ ETHERSCAN_API_KEY found: {masked_key}")
            else:
                log.warning("‚ö†Ô∏è  ETHERSCAN_API_KEY not set")
                log.warning("   Will use basic graph-based analysis instead")
                log.warning("   Add ETHERSCAN_API_KEY=your_key to .env file for full functionality")
            log.info("")
        
        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞—Ñ
            self.load_graph()
            
            # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã –∞–¥—Ä–µ—Å–æ–≤
            self.detect_wallet_groups()
            
            if not self.wallet_groups:
                log.error("No wallet groups found")
                return
            
            # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
            self.load_or_analyze_individual_addresses()
            
            # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø
            self.calculate_group_statistics()
            
            if not self.group_stats:
                log.error("No group statistics calculated")
                return
            
            python# 5. –°–æ–∑–¥–∞–µ–º —Ç–æ–ª—å–∫–æ PNG –≥—Ä–∞—Ñ–∏–∫–∏ (–ë–ï–ó HTML)
            log.info("Creating static PNG visualizations...")
            self.create_transaction_volume_bins_for_all_groups()  # –ù–û–í–´–ô: bin plot –ø–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º
            self.create_group_volume_distribution()
            self.create_group_size_analysis()
            self.create_top_groups_analysis()
            self.create_groups_vs_individuals_comparison()

            # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON/CSV
            json_file, csv_file = self.save_groups_data()

            # 7. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥ (–ë–ï–ó HTML —Ñ–∞–π–ª–∞)
            log.info("=" * 60)
            log.info("GROUPS ANALYSIS COMPLETED SUCCESSFULLY")
            log.info("=" * 60)
            log.info(f"üìä {stats['total_groups']} groups analyzed")
            log.info(f"üë• {stats['total_addresses_in_groups']} addresses in groups")
            log.info(f"üí∞ ${stats['total_volume']:,.0f} total volume")
            log.info(f"üîÑ {stats['total_transactions']:,.0f} total transactions")
            log.info(f"üìÅ Charts saved to: {self.output_dir}/plots/")
            log.info(f"üìÅ Data saved to: {json_file} and {csv_file}")
            log.info("üìà NEW: Transaction volume bins chart created")
            log.info("üìà Generated 5 PNG charts (no HTML reports)")
            log.info("=" * 60)
            
        except Exception as e:
            log.error(f"Groups analysis failed: {e}")
            import traceback
            log.error(traceback.format_exc())
            raise

    def save_groups_data(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø –≤ JSON –∏ CSV"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
        if FULL_ANALYSIS_AVAILABLE and not (self.addresses_file_path and self.addresses_file_path.exists()):
            prefix = "groups_full_analysis_365d"
        else:
            prefix = "groups_analysis"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
        json_file = self.output_dir / "data" / f"{prefix}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json_data = [asdict(group) for group in self.group_stats]
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
        csv_file = self.output_dir / "data" / f"{prefix}_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            if self.group_stats:
                writer = csv.DictWriter(f, fieldnames=asdict(self.group_stats[0]).keys())
                writer.writeheader()
                for group in self.group_stats:
                    writer.writerow(asdict(group))
        
        log.info(f"Groups data saved to: {json_file} and {csv_file}")
        return json_file, csv_file

    def _calculate_bins(self, data, bins):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –±–∏–Ω–∞—Ö"""
        bin_counts = []
        bin_labels = []
        
        for min_val, max_val, label in bins:
            if max_val == float('inf'):
                count = len([x for x in data if x >= min_val])
            else:
                count = len([x for x in data if min_val <= x < max_val])
            
            bin_counts.append(count)
            bin_labels.append(label)
        
        return bin_counts, bin_labels

    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø"""
        log.info("=" * 60)
        log.info("ROCKET POOL GROUPS ANALYSIS STARTED")
        log.info("=" * 60)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –∞–Ω–∞–ª–∏–∑–∞
        if FULL_ANALYSIS_AVAILABLE and not (self.addresses_file_path and self.addresses_file_path.exists()):
            log.info("üöÄ FULL ANALYSIS MODE:")
            log.info("   ‚Ä¢ Will fetch 365-day detailed statistics via APIs")
            log.info("   ‚Ä¢ Uses Etherscan API for transaction history") 
            log.info("   ‚Ä¢ Uses Coinbase API for historical token prices")
            log.info("   ‚Ä¢ Includes gas fees, wallet age, activity patterns")
            log.info("")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
            etherscan_api_key = os.getenv("ETHERSCAN_API_KEY")
            if etherscan_api_key:
                masked_key = etherscan_api_key[:8] + "..." + etherscan_api_key[-4:] if len(etherscan_api_key) > 12 else "***"
                log.info(f"‚úÖ ETHERSCAN_API_KEY found: {masked_key}")
            else:
                log.warning("‚ö†Ô∏è  ETHERSCAN_API_KEY not set")
                log.warning("   Will use basic graph-based analysis instead")
                log.warning("   Add ETHERSCAN_API_KEY=your_key to .env file for full functionality")
            log.info("")
        
        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞—Ñ
            self.load_graph()
            
            # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã –∞–¥—Ä–µ—Å–æ–≤
            self.detect_wallet_groups()
            
            if not self.wallet_groups:
                log.error("No wallet groups found")
                return
            
            # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤
            self.load_or_analyze_individual_addresses()
            
            # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø
            self.calculate_group_statistics()
            
            if not self.group_stats:
                log.error("No group statistics calculated")
                return
            
            # 5. –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            self.create_group_volume_distribution()
            self.create_group_size_analysis()
            self.create_top_groups_analysis()
            self.create_groups_vs_individuals_comparison()
            
            # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            json_file, csv_file = self.save_groups_data()
            
            # 7. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            stats = self.generate_groups_report()
            
            log.info("=" * 60)
            log.info("GROUPS ANALYSIS COMPLETED SUCCESSFULLY")
            log.info("=" * 60)
            log.info(f"üìä {stats['total_groups']} groups analyzed")
            log.info(f"üë• {stats['total_addresses_in_groups']} addresses in groups")
            log.info(f"üí∞ ${stats['total_volume']:,.0f} total volume")
            log.info(f"üìÅ Results saved to: {self.output_dir}")
            log.info("=" * 60)
            
        except Exception as e:
            log.error(f"Groups analysis failed: {e}")
            import traceback
            log.error(traceback.format_exc())
            raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Rocket Pool Groups Analyzer - analyzes coordinated groups of addresses with FULL 365-day API statistics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
üöÄ FULL ANALYSIS FEATURES:
‚Ä¢ Detects coordinated wallet groups using advanced algorithms
‚Ä¢ Fetches 365-day detailed statistics via Etherscan API  
‚Ä¢ Uses historical token prices from Coinbase API
‚Ä¢ Includes gas fees, wallet age, activity patterns
‚Ä¢ Creates comprehensive visualizations and reports

üìã REQUIREMENTS & SETUP:
‚Ä¢ ETHERSCAN_API_KEY for full functionality
‚Ä¢ Internet connection for API calls
‚Ä¢ Run from mintegrity project root directory

üîë API KEY SETUP (choose one method):
Method 1 - .env file (recommended):
  echo "ETHERSCAN_API_KEY=your_api_key_here" >> .env
  
Method 2 - Environment variable:
  export ETHERSCAN_API_KEY="your_api_key_here"
  
Method 3 - Terminal session:
  ETHERSCAN_API_KEY="your_api_key" python scripts/stats_vis/rocket_pool_groups_analyzer.py

Get free API key: https://etherscan.io/apis

üöÄ EXAMPLES:
From mintegrity root (recommended):
  cd mintegrity && python scripts/stats_vis/rocket_pool_groups_analyzer.py
  cd mintegrity && python scripts/stats_vis/rocket_pool_groups_analyzer.py --threshold 6.0

From scripts/stats_vis/ directory:
  python rocket_pool_groups_analyzer.py --graph-path ../../files/rocket_pool_full_graph_90_days.json
  python rocket_pool_groups_analyzer.py --graph-path ../../files/custom_graph.json --threshold 3.0

With custom settings:
  python scripts/stats_vis/rocket_pool_groups_analyzer.py --min-group-size 3 --max-workers 10
  python scripts/stats_vis/rocket_pool_groups_analyzer.py --addresses-file files/existing_analysis.json

Note: 
- Script automatically detects correct paths
- Without API key: basic analysis using graph data only
- With API key: full 365-day analysis with USD values, gas fees, etc.
- Script automatically loads .env file from project root
        """
    )
    
    parser.add_argument(
        "--graph-path",
        default="files/rocket_pool_full_graph_90_days.json",
        help="Path to graph file (default: files/rocket_pool_full_graph_90_days.json)"
    )
    
    parser.add_argument(
        "--addresses-file",
        help="Path to existing addresses analysis JSON file (optional)"
    )
    
    parser.add_argument(
        "--output-dir",
        default="files/rocket_pool_groups_analysis",
        help="Output directory (default: files/rocket_pool_groups_analysis)"
    )
    
    parser.add_argument(
        "--threshold",
        type=float,
        default=5.0,
        help="Coordination threshold for grouping (default: 5.0)"
    )
    
    parser.add_argument(
        "--min-group-size",
        type=int,
        default=2,
        help="Minimum group size (default: 2)"
    )
    
    parser.add_argument(
        "--max-workers",
        type=int,
        default=5,
        help="Maximum concurrent API requests (default: 5)"
    )
    
    args = parser.parse_args()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º
    log.info(f"Working directory: {Path.cwd()}")
    
    try:
        analyzer = RocketPoolGroupsAnalyzer(
            graph_file_path=args.graph_path,
            addresses_file_path=args.addresses_file,
            output_dir=args.output_dir,
            coordination_threshold=args.threshold,
            min_group_size=args.min_group_size,
            max_workers=args.max_workers
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≥—Ä–∞—Ñ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if not analyzer.graph_file_path.exists():
            log.error(f"Graph file not found: {analyzer.graph_file_path}")
            log.error("Solutions:")
            log.error("1. Run from mintegrity root: cd /path/to/mintegrity")
            log.error("2. Use correct relative path: --graph-path ../../files/graph.json")
            log.error("3. Use absolute path: --graph-path /full/path/to/graph.json")
            return 1
        
        analyzer.run_full_analysis()
        
    except KeyboardInterrupt:
        log.info("Analysis interrupted by user")
        return 1
    except Exception as e:
        log.error(f"Analysis failed: {e}")
        log.error("If you're getting import errors, make sure you're running from the mintegrity project root")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
